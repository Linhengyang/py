pytorch 的默认执行是 eager execution: define-by-run, 即 动态执行, 就像Python解释运行一样, 逐行运行每一条torch py code
优点是对 dynamic control flow (if/loop) 很友好, 原生 python object 无缝运行
缺点是慢, 一方面 python解释器有限制, 另一方面是 network 计算图没有全局优化

graph-based execution: 整体构建 network 计算图 以充分优化，易于 port 到非python环境。类似AOT编译
缺点就是 eager execution反过来，很难调试，很难控制 动态控制流








混合编程 hybrid program
for pytorch 1, 解决方案叫 torchscript: pytorch 的一个可以被完整编译成 静态计算图 IR 的子集, 并在runtime对这个IR作各种优化
    生成IR有两种方法：

    方法1: tracing, torch.jit.trace, 提供一 batch of input data, torchscript 记录 实际执行过程并生成计算图
    好坏处都是显而易见的: 好处就是简单确定, 坏处就是 for graph operations based on input data, 无法全部捕捉，即还是很难处理动态控制流

    方法2: scripting, 装饰 torch.jit.script 到 nn.Module类, torchsrcipt 会分析源代码, 生成计算图
    好处是它能处理部分 Python 原生控制流(if/loop), 缺点是不是所有Python code都支持script. 需要针对性改动源代码以 scriptable

介绍一下 torchscript: 请看 script.ipynb











for pytorch 2, 解决方案叫 torch.compile。这是一个多项新技术的综合方案，包括 torchDynamo / torch.FX / torchInductor / AOT auto-grad
大概解释各个概念:
    第一步: torchDynamo, JIT编译器, 通过理解python字节码, 把 pytorch 涉及的部分翻译成 FX graph。
    对于不可编译成 graph 的部分（比如外部 library 调用、复杂的dynamic control flow等不可被静态表示的），它可以退出编译，以eager execution
    的办法执行这部分后，再开始对剩下的部分尝试编译。      -------------------> torchDynamo, 一个可动态退出的JIT编译前端

    第一步的结果: torch.FX(functional eXchange), IR  

    核心理念： PyTorch FX 是一个用于程序捕获和转换的工具包，专注于在 Python 层面安全、可靠地获取和操作 PyTorch 模型（特别是 nn.Module 实例）的中间表示。
    它是一个更底层的、面向开发者的工具，是 PyTorch 2.x 中 torch.compile 的基石之一。

    作用：
    符号化跟踪 (Symbolic Tracing)： FX 的核心是 torch.fx.symbolic_trace。与 TorchScript 的跟踪不同，FX 的符号化跟踪能够更智能地处理 Python 语言特性，
        并构建一个名为 Graph 的中间表示。这个 Graph 包含了一系列 Node，每个 Node 代表一个 PyTorch 操作或 Python 函数调用。
    中间表示 (Intermediate Representation, IR)： FX 生成的 Graph 是一种高级、易于理解和操作的 IR，它保留了原始 PyTorch 代码的结构，
        比如 nn.Module 的调用会作为图中的一个节点，而不是将其内部操作全部展开。这使得开发者可以方便地对模型图进行：
            转换 (Transformation)： 例如，模型量化 (Quantization)、图优化 (Graph Optimization)、算子融合 (Operator Fusion) 等。
            分析 (Analysis)： 比如，识别模型中的特定模式。
    Python 代码生成 (Python Code Generation)： FX 可以将一个 Graph 转换回可执行的 Python 代码，生成一个新的 nn.Module 实例（称为 GraphModule）。
        这是 FX 被称为 "Python-to-Python" 转换工具的原因。这意味着通过 FX 转换后的模型仍然是普通的 PyTorch Python 模型，可以继续在 Eager 模式下运行，也可以再次被其他工具处理。

    组成部分：
    SymbolicTracer： 负责将 nn.Module 的 forward 方法转换为 Graph。
    Graph： 核心的 IR，由一系列 Node 组成。（开发者可以在这一步修改 Graph）。
    GraphModule： 一个特殊的 nn.Module 子类，它包含一个 Graph，并且其 forward 方法是根据这个 Graph 动态生成的 Python 代码（动态生成是指把图重新写成Pytorch代码）。


    第二步: torchInductor, 编译后端, 把 FX graph 编译成 optimized kernels 即高性能代码. 默认的后端是 Triton, 可以理解为跑在 cuda 上的 python

    auto-grad: 构建反向转播图






compile 和 scripting/tracing 的不同点

对于 compile 方法，torch.compile 是对原 eager-mode model 的一个wrapper：所有参数 params 依然保存在 原 eager-mode model instance 中。
torch.compile解决的是“你代码写得不够好”的问题：
    1. 它先通过 torchDynamo，理解python字节码，识别出 可编译部分 和 不可编译部分。对于 不可编译部分，它将以 eager-mode 执行。可编译部分就是一个个 子图，进入下一步
    2. 使用 FX 去中间表示这些 子图。表示的过程分三步，
        1. 符号化跟踪 (Symbolic Tracing)子图，生成一个中间表示 Graph
        2. 开发者可以对 Graph 作修改
        3. FX可以将这个 Graph 重新转换成 Python 代码，重新解释成一个熟悉的 nn.Module 对象（叫GraphModule）。由于是机器从 Graph 翻译过来的，所以它是纯 Pytorch操作
    3. 最后使用 torchInductor，把上一步得到的 Graph 编译成 高性能代码（是Triton这样的DSL还是机器码，不太清楚细节）

归纳一下，torch.compile 是通过复杂的图/子图捉取、代码生成，来提升训练和推理的性能，模型参数仍然被 original nn.Module 管理。


对于 script/trace 方法，torch.jit.script/torch.jit.tracing：
对原 eager-mode model 的源代码 or python字节码（original nn.Module）作静态分析，JIT编译成一个全新的 torch.jit.scriptModule 对象。
这个对象是 original nn.Module 的解耦的、独立的、可序列化的中间表示。它自身就保管了所有参数（从original nn.Module复制），且使用C++ backed graph。所以它可以被移植到非Python
环境。torch.jit.script/torch.jit.tracing 解决的是“你的工具python解释器太慢”的问题。