pytorch 的默认执行是 eager execution: define-by-run, 即 动态执行, 就像Python解释运行一样, 逐行运行每一条torch py code
优点是对 dynamic control flow (if/loop) 很友好, 原生 python object 无缝运行
缺点是慢, 一方面 python解释器有限制, 另一方面是 network 计算图没有全局优化

graph-based execution: 整体构建 network 计算图 以充分优化，易于 port 到非python环境。类似AOT编译
缺点就是 eager execution反过来，很难调试，很难控制 动态控制流


混合编程 hybrid program
for pytorch 1, 解决方案叫 torchscript: pytorch 的一个可以被完整编译成 静态计算图 IR 的子集, 并在runtime对这个IR作各种优化
    生成IR有两种方法：

    方法1: tracing, torch.jit.trace, 提供一 batch of input data, torchscript 记录 实际执行过程并生成计算图
    好坏处都是显而易见的: 好处就是简单确定, 坏处就是 for graph operations based on input data, 无法全部捕捉，即还是很难处理动态控制流

    方法2: scripting, 装饰 torch.jit.script 到 nn.Module类, torchsrcipt 会分析源代码, 生成计算图
    好处是它能处理部分 Python 原生控制流(if/loop), 缺点是不是所有Python code都支持script. 需要针对性改动源代码以 scriptable

介绍一下 torchscript: script.ipynb











for pytorch 2, 解决方案叫 torch.compile。这是一个多项新技术的综合方案，包括 torchDynamo / torch.FX / torchInductor / AOT auto-grad
大概解释各个概念:
    第一步: torchDynamo, JIT编译器, 通过理解python字节码, 把 pytorch 涉及的部分翻译成 FX graph。
    对于不可编译成 graph 的部分（比如外部 library 调用、复杂的dynamic control flow等不可被静态表示的），它可以退出编译，以eager execution
    的办法执行这部分后，再开始对剩下的部分尝试编译。      -------------------> torchDynamo, 一个可动态退出的JIT编译前端

    第二步: torch.FX(functional eXchange), IR

    第三步: torchInductor, 编译后端, 把 FX graph 编译成 optimized kernels. 默认的后端是 Triton, 可以理解为跑在 cuda 上的 python

    auto-grad: 构建反向转播图
